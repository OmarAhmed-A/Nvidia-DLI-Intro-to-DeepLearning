{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on going through today's course! Hopefully, you've learned some valuable skills along the way and had fun doing it. Now it's time to put those skills to the test. In this assessment, you will train a new model that is able to recognize fresh and rotten fruit. You will need to get the model to a validation accuracy of `92%` in order to pass the assessment, though we challenge you to do even better if you can. You will have the use the skills that you learned in the previous exercises. Specifically, we suggest using some combination of transfer learning, data augmentation, and fine tuning. Once you have trained the model to be at least 92% accurate on the validation dataset, save your model, and then assess its accuracy. Let's get started! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will train a model to recognize fresh and rotten fruits. The dataset comes from [Kaggle](https://www.kaggle.com/sriramr/fruits-fresh-and-rotten-for-classification), a great place to go if you're interested in starting a project after this class. The dataset structure is in the `data/fruits` folder. There are 6 categories of fruits: fresh apples, fresh oranges, fresh bananas, rotten apples, rotten oranges, and rotten bananas. This will mean that your model will require an output layer of 6 neurons to do the categorization successfully. You'll also need to compile the model with `categorical_crossentropy`, as we have more than two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fruits.png\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ImageNet Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage you to start with a model pretrained on ImageNet. Load the model with the correct weights, set an input shape, and choose to remove the last layers of the model. Remember that images have three dimensions: a height, and width, and a number of channels. Because these pictures are in color, there will be three channels for red, green, and blue. We've filled in the input shape for you. This cannot be changed or the assessment will fail. If you need a reference for setting up the pretrained model, please take a look at [notebook 05b](05b_presidential_doggy_door.ipynb) where we implemented transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "base_model = keras.applications.VGG16(\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we suggest freezing the base model, as done in [notebook 05b](05b_presidential_doggy_door.ipynb). This is done so that all the learning from the ImageNet dataset does not get destroyed in the initial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Layers to Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to add layers to the pretrained model. [Notebook 05b](05b_presidential_doggy_door.ipynb) can be used as a guide. Pay close attention to the last dense layer and make sure it has the correct number of neurons to classify the different types of fruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inputs with correct shape\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "x = base_model(inputs, training=False)\n",
    "\n",
    "# Add pooling layer or flatten layer\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add final dense layer\n",
    "outputs = keras.layers.Dense(6, activation = 'softmax')(x)\n",
    "\n",
    "# Combine inputs and outputs to create model\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 14,717,766\n",
      "Trainable params: 3,078\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to compile the model with loss and metrics options. Remember that we're training on a number of different categories, rather than a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.CategoricalCrossentropy() , metrics = keras.metrics.CategoricalAccuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like, try to augment the data to improve the dataset. Feel free to look at [notebook 04a](04a_asl_augmentation.ipynb) and [notebook 05b](05b_presidential_doggy_door.ipynb) for augmentation examples. There is also documentation for the [Keras ImageDataGenerator class](https://keras.io/api/preprocessing/image/#imagedatagenerator-class). This step is optional, but it may be helpful to get to 92% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen_train = datagen_train = ImageDataGenerator(\n",
    "    samplewise_center=True,  # set each sample mean to 0\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range=0.1,  # Randomly zoom image\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=True,\n",
    ")\n",
    "datagen_valid = ImageDataGenerator(samplewise_center=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to load the train and validation datasets. Pick the right folders, as well as the right `target_size` of the images (it needs to match the height and width input of the model you've created). For a reference, check out [notebook 05b](05b_presidential_doggy_door.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1182 images belonging to 6 classes.\n",
      "Found 329 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# load and iterate training dataset\n",
    "train_it = datagen_train.flow_from_directory(\n",
    "    'data/fruits/train',\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "# load and iterate validation dataset\n",
    "valid_it = datagen_valid.flow_from_directory(\n",
    "    'data/fruits/valid',\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    class_mode=\"categorical\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train the model! Pass the `train` and `valid` iterators into the `fit` function, as well as setting the desired number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "37/36 [==============================] - 17s 452ms/step - loss: 0.0522 - categorical_accuracy: 0.9788 - val_loss: 0.1978 - val_categorical_accuracy: 0.9544\n",
      "Epoch 2/5\n",
      "37/36 [==============================] - 17s 456ms/step - loss: 0.0485 - categorical_accuracy: 0.9822 - val_loss: 0.1883 - val_categorical_accuracy: 0.9605\n",
      "Epoch 3/5\n",
      "37/36 [==============================] - 17s 461ms/step - loss: 0.0488 - categorical_accuracy: 0.9856 - val_loss: 0.2260 - val_categorical_accuracy: 0.9392\n",
      "Epoch 4/5\n",
      "37/36 [==============================] - 17s 455ms/step - loss: 0.0340 - categorical_accuracy: 0.9882 - val_loss: 0.1792 - val_categorical_accuracy: 0.9574\n",
      "Epoch 5/5\n",
      "37/36 [==============================] - 17s 453ms/step - loss: 0.0350 - categorical_accuracy: 0.9856 - val_loss: 0.1718 - val_categorical_accuracy: 0.9605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f78d699c2e8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_it,\n",
    "          validation_data=valid_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "          epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfreeze Model for Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have reached 92% validation accuracy already, this next step is optional. If not, we suggest fine tuning the model with a very low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Compile the model with a low learning rate\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate = 0.0001),\n",
    "              loss = keras.losses.CategoricalCrossentropy() , metrics = keras.metrics.CategoricalAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_it,\n",
    "          validation_data=valid_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "          epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you now have a model that has a validation accuracy of 92% or higher. If not, you may want to go back and either run more epochs of training, or adjust your data augmentation. \n",
    "\n",
    "Once you are satisfied with the validation accuracy, evaluate the model by executing the following cell. The evaluate function will return a tuple, where the first value is your loss, and the second value is your accuracy. To pass, the model will need have an accuracy value of `92% or higher`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10 [================================] - 2s 141ms/step - loss: 0.1905 - categorical_accuracy: 0.9635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.19051319360733032, 0.9635258316993713]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_it, steps=valid_it.samples/valid_it.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess your model run the following two cells.\n",
    "\n",
    "**NOTE:** `run_assessment` assumes your model is named `model` and your validation data iterator is called `valid_it`. If for any reason you have modified these variable names, please update the names of the arguments passed to `run_assessment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_assessment import run_assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model 5 times to obtain average accuracy...\n",
      "\n",
      "11/10 [================================] - 2s 137ms/step - loss: 0.1765 - categorical_accuracy: 0.9574\n",
      "11/10 [================================] - 1s 133ms/step - loss: 0.2023 - categorical_accuracy: 0.9544\n",
      "11/10 [================================] - 1s 135ms/step - loss: 0.1218 - categorical_accuracy: 0.9726\n",
      "11/10 [================================] - 2s 140ms/step - loss: 0.1432 - categorical_accuracy: 0.9574\n",
      "11/10 [================================] - 2s 137ms/step - loss: 0.2169 - categorical_accuracy: 0.9605\n",
      "\n",
      "Accuracy required to pass the assessment is 0.92 or greater.\n",
      "Your average accuracy is 0.9605.\n",
      "\n",
      "Congratulations! You passed the assessment!\n",
      "See instructions below to generate a certificate.\n"
     ]
    }
   ],
   "source": [
    "run_assessment(model, valid_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Certificate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you passed the assessment, please return to the course page (shown below) and click the \"ASSESS TASK\" button, which will generate your certificate for the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/assess_task.png\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Cowardly refusing to create an empty archive\n",
      "Try 'tar --help' or 'tar --usage' for more information.\n"
     ]
    }
   ],
   "source": [
    "!tar chvfz notebook.tar.gz ./*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: tar [OPTION...] [FILE]...\n",
      "GNU 'tar' saves many files together into a single tape or disk archive, and can\n",
      "restore individual files from the archive.\n",
      "\n",
      "Examples:\n",
      "  tar -cf archive.tar foo bar  # Create archive.tar from files foo and bar.\n",
      "  tar -tvf archive.tar         # List all files in archive.tar verbosely.\n",
      "  tar -xf archive.tar          # Extract all files from archive.tar.\n",
      "\n",
      " Local file name selection:\n",
      "\n",
      "      --add-file=FILE        add given FILE to the archive (useful if its name\n",
      "                             starts with a dash)\n",
      "  -C, --directory=DIR        change to directory DIR\n",
      "      --exclude=PATTERN      exclude files, given as a PATTERN\n",
      "      --exclude-backups      exclude backup and lock files\n",
      "      --exclude-caches       exclude contents of directories containing\n",
      "                             CACHEDIR.TAG, except for the tag file itself\n",
      "      --exclude-caches-all   exclude directories containing CACHEDIR.TAG\n",
      "      --exclude-caches-under exclude everything under directories containing\n",
      "                             CACHEDIR.TAG\n",
      "      --exclude-ignore=FILE  read exclude patterns for each directory from\n",
      "                             FILE, if it exists\n",
      "      --exclude-ignore-recursive=FILE\n",
      "                             read exclude patterns for each directory and its\n",
      "                             subdirectories from FILE, if it exists\n",
      "      --exclude-tag=FILE     exclude contents of directories containing FILE,\n",
      "                             except for FILE itself\n",
      "      --exclude-tag-all=FILE exclude directories containing FILE\n",
      "      --exclude-tag-under=FILE   exclude everything under directories\n",
      "                             containing FILE\n",
      "      --exclude-vcs          exclude version control system directories\n",
      "      --exclude-vcs-ignores  read exclude patterns from the VCS ignore files\n",
      "      --no-null              disable the effect of the previous --null option\n",
      "      --no-recursion         avoid descending automatically in directories\n",
      "      --no-unquote           do not unquote input file or member names\n",
      "      --no-verbatim-files-from   -T treats file names starting with dash as\n",
      "                             options (default)\n",
      "      --null                 -T reads null-terminated names; implies\n",
      "                             --verbatim-files-from\n",
      "      --recursion            recurse into directories (default)\n",
      "  -T, --files-from=FILE      get names to extract or create from FILE\n",
      "      --unquote              unquote input file or member names (default)\n",
      "      --verbatim-files-from  -T reads file names verbatim (no option handling)\n",
      "  -X, --exclude-from=FILE    exclude patterns listed in FILE\n",
      "\n",
      " File name matching options (affect both exclude and include patterns):\n",
      "\n",
      "      --anchored             patterns match file name start\n",
      "      --ignore-case          ignore case\n",
      "      --no-anchored          patterns match after any '/' (default for\n",
      "                             exclusion)\n",
      "      --no-ignore-case       case sensitive matching (default)\n",
      "      --no-wildcards         verbatim string matching\n",
      "      --no-wildcards-match-slash   wildcards do not match '/'\n",
      "      --wildcards            use wildcards (default for exclusion)\n",
      "      --wildcards-match-slash   wildcards match '/' (default for exclusion)\n",
      "\n",
      " Main operation mode:\n",
      "\n",
      "  -A, --catenate, --concatenate   append tar files to an archive\n",
      "  -c, --create               create a new archive\n",
      "  -d, --diff, --compare      find differences between archive and file system\n",
      "      --delete               delete from the archive (not on mag tapes!)\n",
      "  -r, --append               append files to the end of an archive\n",
      "  -t, --list                 list the contents of an archive\n",
      "      --test-label           test the archive volume label and exit\n",
      "  -u, --update               only append files newer than copy in archive\n",
      "  -x, --extract, --get       extract files from an archive\n",
      "\n",
      " Operation modifiers:\n",
      "\n",
      "      --check-device         check device numbers when creating incremental\n",
      "                             archives (default)\n",
      "  -g, --listed-incremental=FILE   handle new GNU-format incremental backup\n",
      "  -G, --incremental          handle old GNU-format incremental backup\n",
      "      --hole-detection=TYPE  technique to detect holes\n",
      "      --ignore-failed-read   do not exit with nonzero on unreadable files\n",
      "      --level=NUMBER         dump level for created listed-incremental archive\n",
      "  -n, --seek                 archive is seekable\n",
      "      --no-check-device      do not check device numbers when creating\n",
      "                             incremental archives\n",
      "      --no-seek              archive is not seekable\n",
      "      --occurrence[=NUMBER]  process only the NUMBERth occurrence of each file\n",
      "                             in the archive; this option is valid only in\n",
      "                             conjunction with one of the subcommands --delete,\n",
      "                             --diff, --extract or --list and when a list of\n",
      "                             files is given either on the command line or via\n",
      "                             the -T option; NUMBER defaults to 1\n",
      "      --sparse-version=MAJOR[.MINOR]\n",
      "                             set version of the sparse format to use (implies\n",
      "                             --sparse)\n",
      "  -S, --sparse               handle sparse files efficiently\n",
      "\n",
      " Overwrite control:\n",
      "\n",
      "  -k, --keep-old-files       don't replace existing files when extracting,\n",
      "                             treat them as errors\n",
      "      --keep-directory-symlink   preserve existing symlinks to directories when\n",
      "                             extracting\n",
      "      --keep-newer-files     don't replace existing files that are newer than\n",
      "                             their archive copies\n",
      "      --no-overwrite-dir     preserve metadata of existing directories\n",
      "      --one-top-level[=DIR]  create a subdirectory to avoid having loose files\n",
      "                             extracted\n",
      "      --overwrite            overwrite existing files when extracting\n",
      "      --overwrite-dir        overwrite metadata of existing directories when\n",
      "                             extracting (default)\n",
      "      --recursive-unlink     empty hierarchies prior to extracting directory\n",
      "      --remove-files         remove files after adding them to the archive\n",
      "      --skip-old-files       don't replace existing files when extracting,\n",
      "                             silently skip over them\n",
      "  -U, --unlink-first         remove each file prior to extracting over it\n",
      "  -W, --verify               attempt to verify the archive after writing it\n",
      "\n",
      " Select output stream:\n",
      "\n",
      "      --ignore-command-error ignore exit codes of children\n",
      "      --no-ignore-command-error   treat non-zero exit codes of children as\n",
      "                             error\n",
      "  -O, --to-stdout            extract files to standard output\n",
      "      --to-command=COMMAND   pipe extracted files to another program\n",
      "\n",
      " Handling of file attributes:\n",
      "\n",
      "      --atime-preserve[=METHOD]   preserve access times on dumped files, either\n",
      "                             by restoring the times after reading\n",
      "                             (METHOD='replace'; default) or by not setting the\n",
      "                             times in the first place (METHOD='system')\n",
      "      --clamp-mtime          only set time when the file is more recent than\n",
      "                             what was given with --mtime\n",
      "      --delay-directory-restore   delay setting modification times and\n",
      "                             permissions of extracted directories until the end\n",
      "                             of extraction\n",
      "      --group=NAME           force NAME as group for added files\n",
      "      --group-map=FILE       use FILE to map file owner GIDs and names\n",
      "      --mode=CHANGES         force (symbolic) mode CHANGES for added files\n",
      "      --mtime=DATE-OR-FILE   set mtime for added files from DATE-OR-FILE\n",
      "  -m, --touch                don't extract file modified time\n",
      "      --no-delay-directory-restore\n",
      "                             cancel the effect of --delay-directory-restore\n",
      "                             option\n",
      "      --no-same-owner        extract files as yourself (default for ordinary\n",
      "                             users)\n",
      "      --no-same-permissions  apply the user's umask when extracting permissions\n",
      "                             from the archive (default for ordinary users)\n",
      "      --numeric-owner        always use numbers for user/group names\n",
      "      --owner=NAME           force NAME as owner for added files\n",
      "      --owner-map=FILE       use FILE to map file owner UIDs and names\n",
      "  -p, --preserve-permissions, --same-permissions\n",
      "                             extract information about file permissions\n",
      "                             (default for superuser)\n",
      "      --same-owner           try extracting files with the same ownership as\n",
      "                             exists in the archive (default for superuser)\n",
      "  -s, --preserve-order, --same-order\n",
      "                             member arguments are listed in the same order as\n",
      "                             the files in the archive\n",
      "      --sort=ORDER           directory sorting order: none (default), name or\n",
      "                             inode\n",
      "\n",
      " Handling of extended file attributes:\n",
      "\n",
      "      --acls                 Enable the POSIX ACLs support\n",
      "      --no-acls              Disable the POSIX ACLs support\n",
      "      --no-selinux           Disable the SELinux context support\n",
      "      --no-xattrs            Disable extended attributes support\n",
      "      --selinux              Enable the SELinux context support\n",
      "      --xattrs               Enable extended attributes support\n",
      "      --xattrs-exclude=MASK  specify the exclude pattern for xattr keys\n",
      "      --xattrs-include=MASK  specify the include pattern for xattr keys\n",
      "\n",
      " Device selection and switching:\n",
      "\n",
      "  -f, --file=ARCHIVE         use archive file or device ARCHIVE\n",
      "      --force-local          archive file is local even if it has a colon\n",
      "  -F, --info-script=NAME, --new-volume-script=NAME\n",
      "                             run script at end of each tape (implies -M)\n",
      "  -L, --tape-length=NUMBER   change tape after writing NUMBER x 1024 bytes\n",
      "  -M, --multi-volume         create/list/extract multi-volume archive\n",
      "      --rmt-command=COMMAND  use given rmt COMMAND instead of rmt\n",
      "      --rsh-command=COMMAND  use remote COMMAND instead of rsh\n",
      "      --volno-file=FILE      use/update the volume number in FILE\n",
      "\n",
      " Device blocking:\n",
      "\n",
      "  -b, --blocking-factor=BLOCKS   BLOCKS x 512 bytes per record\n",
      "  -B, --read-full-records    reblock as we read (for 4.2BSD pipes)\n",
      "  -i, --ignore-zeros         ignore zeroed blocks in archive (means EOF)\n",
      "      --record-size=NUMBER   NUMBER of bytes per record, multiple of 512\n",
      "\n",
      " Archive format selection:\n",
      "\n",
      "  -H, --format=FORMAT        create archive of the given format\n",
      "\n",
      " FORMAT is one of the following:\n",
      "\n",
      "    gnu                      GNU tar 1.13.x format\n",
      "    oldgnu                   GNU format as per tar <= 1.12\n",
      "    pax                      POSIX 1003.1-2001 (pax) format\n",
      "    posix                    same as pax\n",
      "    ustar                    POSIX 1003.1-1988 (ustar) format\n",
      "    v7                       old V7 tar format\n",
      "\n",
      "      --old-archive, --portability\n",
      "                             same as --format=v7\n",
      "      --pax-option=keyword[[:]=value][,keyword[[:]=value]]...\n",
      "                             control pax keywords\n",
      "      --posix                same as --format=posix\n",
      "  -V, --label=TEXT           create archive with volume name TEXT; at\n",
      "                             list/extract time, use TEXT as a globbing pattern\n",
      "                             for volume name\n",
      "\n",
      " Compression options:\n",
      "\n",
      "  -a, --auto-compress        use archive suffix to determine the compression\n",
      "                             program\n",
      "  -I, --use-compress-program=PROG\n",
      "                             filter through PROG (must accept -d)\n",
      "  -j, --bzip2                filter the archive through bzip2\n",
      "  -J, --xz                   filter the archive through xz\n",
      "      --lzip                 filter the archive through lzip\n",
      "      --lzma                 filter the archive through xz\n",
      "      --lzop                 filter the archive through xz\n",
      "      --no-auto-compress     do not use archive suffix to determine the\n",
      "                             compression program\n",
      "  -z, --gzip, --gunzip, --ungzip   filter the archive through gzip\n",
      "  -Z, --compress, --uncompress   filter the archive through compress\n",
      "\n",
      " Local file selection:\n",
      "\n",
      "      --backup[=CONTROL]     backup before removal, choose version CONTROL\n",
      "  -h, --dereference          follow symlinks; archive and dump the files they\n",
      "                             point to\n",
      "      --hard-dereference     follow hard links; archive and dump the files they\n",
      "                             refer to\n",
      "  -K, --starting-file=MEMBER-NAME\n",
      "                             begin at member MEMBER-NAME when reading the\n",
      "                             archive\n",
      "      --newer-mtime=DATE     compare date and time when data changed only\n",
      "  -N, --newer=DATE-OR-FILE, --after-date=DATE-OR-FILE\n",
      "                             only store files newer than DATE-OR-FILE\n",
      "      --one-file-system      stay in local file system when creating archive\n",
      "  -P, --absolute-names       don't strip leading '/'s from file names\n",
      "      --suffix=STRING        backup before removal, override usual suffix ('~'\n",
      "                             unless overridden by environment variable\n",
      "                             SIMPLE_BACKUP_SUFFIX)\n",
      "\n",
      " File name transformations:\n",
      "\n",
      "      --strip-components=NUMBER   strip NUMBER leading components from file\n",
      "                             names on extraction\n",
      "      --transform=EXPRESSION, --xform=EXPRESSION\n",
      "                             use sed replace EXPRESSION to transform file\n",
      "                             names\n",
      "\n",
      " Informative output:\n",
      "\n",
      "      --checkpoint[=NUMBER]  display progress messages every NUMBERth record\n",
      "                             (default 10)\n",
      "      --checkpoint-action=ACTION   execute ACTION on each checkpoint\n",
      "      --full-time            print file time to its full resolution\n",
      "      --index-file=FILE      send verbose output to FILE\n",
      "  -l, --check-links          print a message if not all links are dumped\n",
      "      --no-quote-chars=STRING   disable quoting for characters from STRING\n",
      "      --quote-chars=STRING   additionally quote characters from STRING\n",
      "      --quoting-style=STYLE  set name quoting style; see below for valid STYLE\n",
      "                             values\n",
      "  -R, --block-number         show block number within archive with each message\n",
      "                            \n",
      "      --show-defaults        show tar defaults\n",
      "      --show-omitted-dirs    when listing or extracting, list each directory\n",
      "                             that does not match search criteria\n",
      "      --show-snapshot-field-ranges\n",
      "                             show valid ranges for snapshot-file fields\n",
      "      --show-transformed-names, --show-stored-names\n",
      "                             show file or archive names after transformation\n",
      "      --totals[=SIGNAL]      print total bytes after processing the archive;\n",
      "                             with an argument - print total bytes when this\n",
      "                             SIGNAL is delivered; Allowed signals are: SIGHUP,\n",
      "                             SIGQUIT, SIGINT, SIGUSR1 and SIGUSR2; the names\n",
      "                             without SIG prefix are also accepted\n",
      "      --utc                  print file modification times in UTC\n",
      "  -v, --verbose              verbosely list files processed\n",
      "      --warning=KEYWORD      warning control\n",
      "  -w, --interactive, --confirmation\n",
      "                             ask for confirmation for every action\n",
      "\n",
      " Compatibility options:\n",
      "\n",
      "  -o                         when creating, same as --old-archive; when\n",
      "                             extracting, same as --no-same-owner\n",
      "\n",
      " Other options:\n",
      "\n",
      "  -?, --help                 give this help list\n",
      "      --restrict             disable use of some potentially harmful options\n",
      "      --usage                give a short usage message\n",
      "      --version              print program version\n",
      "\n",
      "Mandatory or optional arguments to long options are also mandatory or optional\n",
      "for any corresponding short options.\n",
      "\n",
      "The backup suffix is '~', unless set with --suffix or SIMPLE_BACKUP_SUFFIX.\n",
      "The version control may be set with --backup or VERSION_CONTROL, values are:\n",
      "\n",
      "  none, off       never make backups\n",
      "  t, numbered     make numbered backups\n",
      "  nil, existing   numbered if numbered backups exist, simple otherwise\n",
      "  never, simple   always make simple backups\n",
      "\n",
      "Valid arguments for the --quoting-style option are:\n",
      "\n",
      "  literal\n",
      "  shell\n",
      "  shell-always\n",
      "  c\n",
      "  c-maybe\n",
      "  escape\n",
      "  locale\n",
      "  clocale\n",
      "\n",
      "*This* tar defaults to:\n",
      "--format=gnu -f- -b20 --quoting-style=escape --rmt-command=/usr/lib/tar/rmt\n",
      "--rsh-command=/usr/bin/rsh\n"
     ]
    }
   ],
   "source": [
    "!tar --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
